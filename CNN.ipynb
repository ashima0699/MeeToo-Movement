{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled5 (3).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0DqGSSQ1atR"
      },
      "source": [
        "# importing necessary libraries\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.metrics import classification_report,confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXrAohW11eAC"
      },
      "source": [
        "train, test = pd.read_csv('train.csv'), pd.read_csv('test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffAYHMoa1jHE"
      },
      "source": [
        "train.drop(['tweet_id'],axis = 1, inplace=True)\n",
        "test.drop(['tweet_id'],axis = 1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoABnnY8j9_4"
      },
      "source": [
        "\n",
        "pip install wordcloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJIfcO8g1j_2"
      },
      "source": [
        "from nltk.stem.porter import *\n",
        "plt.style.use('seaborn')\n",
        "import plotly.express as px\n",
        "from plotly import graph_objs as go\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfHd03nz2HXs",
        "outputId": "4116f327-05a5-4ad5-888d-0c74a4c9258f"
      },
      "source": [
        "for index, row in train.iterrows():\n",
        "    tweet = row.text\n",
        "    tweet = re.sub(r\"https\\S+\", \"\", tweet) #remove links\n",
        "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) #replace #word with word\n",
        "    \n",
        "    train['text'].iloc[index:index+1] = tweet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_with_indexer(indexer, value)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX6kn05fj9_6"
      },
      "source": [
        "for index, row in test.iterrows():\n",
        "    tweet = row.text\n",
        "    tweet = re.sub(r\"https\\S+\", \"\", tweet) #remove links\n",
        "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) #replace #word with word\n",
        "    \n",
        "    test['text'].iloc[index:index+1] = tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIWqEdDX2Ji1"
      },
      "source": [
        "# Helper function to remove unwanted patterns\n",
        "def remove_pattern(input_txt, pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i, '', input_txt)\n",
        "    return input_txt\n",
        "\n",
        "# Remove Twitter handles from the data \n",
        "train['text'] = np.vectorize(remove_pattern)(train['text'], \"@[\\w]*\")\n",
        "\n",
        "\n",
        "# Remove all words below 2 characters\n",
        "train['text'] = train['text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
        "\n",
        "# Remove punctuations, numbers, and special characters\n",
        "train['text'] = train['text'].str.replace(\"[^a-zA-Z#]\", \" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_0dNFvJj9_7"
      },
      "source": [
        "# Helper function to remove unwanted patterns\n",
        "def remove_pattern(input_txt, pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i, '', input_txt)\n",
        "    return input_txt\n",
        "\n",
        "# Remove Twitter handles from the data \n",
        "test['text'] = np.vectorize(remove_pattern)(test['text'], \"@[\\w]*\")\n",
        "\n",
        "\n",
        "# Remove all words below 2 characters\n",
        "test['text'] = test['text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
        "\n",
        "# Remove punctuations, numbers, and special characters\n",
        "test['text'] = test['text'].str.replace(\"[^a-zA-Z#]\", \" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8v1NhrIj9_7"
      },
      "source": [
        "#tokenize and pad the sentences from processedText matrix\n",
        "max_features = 31000 #number of words we care about\n",
        "sequence_length = 256 #number of words to be taken from each sentence\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>', filters=' ')\n",
        "tokenizer.fit_on_texts(train['text'].values)\n",
        "\n",
        "#create train matrix\n",
        "xTrain = tokenizer.texts_to_sequences(train['text'].values)\n",
        "xTrain = pad_sequences(xTrain, sequence_length)\n",
        "\n",
        "#create text matrix\n",
        "xTest = tokenizer.texts_to_sequences(test['text'].values)\n",
        "xTest = pad_sequences(xTest, sequence_length)\n",
        "#pip install tensorflow\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUBLqn_6j9_7"
      },
      "source": [
        "xTrain = train['text']\n",
        "xTest = test['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kapt0mah2O39"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwrVbSStj9_-"
      },
      "source": [
        "NUM_WORDS=9864\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjSNay-L3SdI",
        "outputId": "4001bece-80b1-4a68-82c8-3637e4fd4179"
      },
      "source": [
        "\n",
        "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',lower=True)\n",
        "tokenizer.fit_on_texts(xTrain)\n",
        "sequences_train = tokenizer.texts_to_sequences(xTrain)\n",
        "sequences_Test=tokenizer.texts_to_sequences(xTest)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10794 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61TTdicE3f9K",
        "outputId": "b6a355b1-6116-4710-a56b-79369ac3f15c"
      },
      "source": [
        "xTrain = pad_sequences(sequences_train)\n",
        "xTest= pad_sequences(sequences_Test, maxlen=xTrain.shape[1])\n",
        "\n",
        "print('Shape of X train and X validation tensor:', xTrain.shape,xTest.shape)\n",
        "##print('Shape of label train and validation tensor:', y_train.shape,y_val.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X train and X validation tensor: (6598, 23) (1648, 23)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip-DZN2sj9__"
      },
      "source": [
        "#pip install -U gensim\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "qSyzfAwB8_JO",
        "outputId": "f5f8a257-9cba-44be-8306-dbe5684a3c33"
      },
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "\n",
        "EMBEDDING_DIM=300\n",
        "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
        "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i>=NUM_WORDS:\n",
        "        continue\n",
        "    try:\n",
        "        embedding_vector = word_vectors[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
        "\n",
        "del(word_vectors)\n",
        "\n",
        "from keras.layers import Embedding\n",
        "embedding_layer = Embedding(vocabulary_size,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01Ck_t1Xj-AA"
      },
      "source": [
        "from keras.layers import Embedding\n",
        "EMBEDDING_DIM=300\n",
        "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
        "\n",
        "embedding_layer = Embedding(vocabulary_size,\n",
        "                            EMBEDDING_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeAfydsyj-AA"
      },
      "source": [
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
        "from keras.layers.core import Reshape, Flatten\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "sequence_length = xTrain.shape[1]\n",
        "filterSizes = [7,8,9]\n",
        "numberOfFilters = 512\n",
        "dropout = 0.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f1C7g-Xj-AB"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "from keras.initializers import Constant\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "##from ipynb.fs.full.datapreprocessing import preProcessDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6uI_ZJJj-AB"
      },
      "source": [
        "embedding_matrix.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvPMFqJXj-AB"
      },
      "source": [
        "NUM_WORDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oksiJcxlj-AC"
      },
      "source": [
        "inputs = Input(shape=(sequence_length,),dtype='int32')\n",
        "embedding = Embedding(NUM_WORDS,EMBEDDING_DIM,embeddings_initializer=Constant(embedding_matrix),\n",
        "                      input_length=sequence_length,trainable=True)(inputs)\n",
        "reshape = Reshape((sequence_length, EMBEDDING_DIM,1))(embedding)\n",
        "\n",
        "conv0 = Conv2D(numberOfFilters, kernel_size=(filterSizes[0], EMBEDDING_DIM), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "conv1 = Conv2D(numberOfFilters, kernel_size=(filterSizes[1], EMBEDDING_DIM), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "conv2 = Conv2D(numberOfFilters, kernel_size=(filterSizes[2], EMBEDDING_DIM), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "\n",
        "maxPool0 = MaxPool2D(pool_size=(sequence_length-filterSizes[0]+1,1), strides=(1,1), padding='valid')(conv0)\n",
        "maxPool1 = MaxPool2D(pool_size=(sequence_length-filterSizes[1]+1,1), strides=(1,1), padding='valid')(conv1)\n",
        "maxPool2 = MaxPool2D(pool_size=(sequence_length-filterSizes[2]+1,1), strides=(1,1), padding='valid')(conv2)\n",
        "\n",
        "concatenatedTensor = Concatenate(axis=1)([maxPool0, maxPool1, maxPool2])\n",
        "flatten = Flatten()(concatenatedTensor)\n",
        "dropout = Dropout(dropout)(flatten)\n",
        "output = Dense(units=1, activation='sigmoid')(dropout)\n",
        "\n",
        "classifier = Model(inputs=inputs, outputs=output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwDZFlTJj-AC",
        "outputId": "71208519-d27e-435c-d0ed-c003095517e5"
      },
      "source": [
        "#train the model and create predictions and store them in a dictionary\n",
        "\n",
        "classes = ['relevance','directed_hate','generalised_hate','sarcasm','allegation','justification','refutation','Favour','oppose']\n",
        "for className in classes:\n",
        "    print('Training and Prediction for the class: ', className)\n",
        "    classifier.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=['acc'])\n",
        "    classifier.fit(xTrain, train[className], batch_size=50, epochs=5, validation_split=0.2)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training and Prediction for the class:  relevance\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 66s 619ms/step - loss: 1.1489 - acc: 0.6396 - val_loss: 0.7209 - val_acc: 0.6962\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 66s 624ms/step - loss: 0.4352 - acc: 0.8041 - val_loss: 0.7730 - val_acc: 0.6833\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 67s 628ms/step - loss: 0.3210 - acc: 0.8713 - val_loss: 0.8092 - val_acc: 0.6561\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 69s 653ms/step - loss: 0.2533 - acc: 0.9105 - val_loss: 0.9096 - val_acc: 0.6598\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 75s 707ms/step - loss: 0.2224 - acc: 0.9272 - val_loss: 0.9661 - val_acc: 0.6705\n",
            "Training and Prediction for the class:  directed_hate\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 73s 677ms/step - loss: 0.4269 - acc: 0.9031 - val_loss: 0.1517 - val_acc: 0.9712\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 67s 635ms/step - loss: 0.0653 - acc: 0.9805 - val_loss: 0.1743 - val_acc: 0.9742\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 67s 631ms/step - loss: 0.0491 - acc: 0.9868 - val_loss: 0.1632 - val_acc: 0.9720\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 67s 634ms/step - loss: 0.0479 - acc: 0.9837 - val_loss: 0.2100 - val_acc: 0.9735\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 68s 640ms/step - loss: 0.0591 - acc: 0.9833 - val_loss: 0.2006 - val_acc: 0.9712\n",
            "Training and Prediction for the class:  generalised_hate\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 68s 631ms/step - loss: 0.1902 - acc: 0.9545 - val_loss: 0.0890 - val_acc: 0.9856\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 79s 749ms/step - loss: 0.0661 - acc: 0.9801 - val_loss: 0.0917 - val_acc: 0.9856\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 73s 687ms/step - loss: 0.0569 - acc: 0.9849 - val_loss: 0.0958 - val_acc: 0.9856\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 69s 654ms/step - loss: 0.0436 - acc: 0.9896 - val_loss: 0.0996 - val_acc: 0.9856\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 68s 638ms/step - loss: 0.0454 - acc: 0.9856 - val_loss: 0.1210 - val_acc: 0.9864\n",
            "Training and Prediction for the class:  sarcasm\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 77s 715ms/step - loss: 0.1852 - acc: 0.9694 - val_loss: 0.0957 - val_acc: 0.9894\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 82s 772ms/step - loss: 0.0723 - acc: 0.9801 - val_loss: 0.0683 - val_acc: 0.9894\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 82s 773ms/step - loss: 0.0456 - acc: 0.9860 - val_loss: 0.0703 - val_acc: 0.9894\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 71s 668ms/step - loss: 0.0339 - acc: 0.9908 - val_loss: 0.0739 - val_acc: 0.9894\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 68s 639ms/step - loss: 0.0353 - acc: 0.9900 - val_loss: 0.0796 - val_acc: 0.9894\n",
            "Training and Prediction for the class:  allegation\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 67s 626ms/step - loss: 0.3602 - acc: 0.9220 - val_loss: 0.1355 - val_acc: 0.9735\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 57s 540ms/step - loss: 0.1312 - acc: 0.9556 - val_loss: 0.1582 - val_acc: 0.9621\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 57s 541ms/step - loss: 0.0914 - acc: 0.9689 - val_loss: 0.1852 - val_acc: 0.9553\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 58s 542ms/step - loss: 0.0807 - acc: 0.9765 - val_loss: 0.1990 - val_acc: 0.9545\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 57s 536ms/step - loss: 0.0760 - acc: 0.9777 - val_loss: 0.1986 - val_acc: 0.9561\n",
            "Training and Prediction for the class:  justification\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 58s 538ms/step - loss: 0.2342 - acc: 0.9516 - val_loss: 0.1665 - val_acc: 0.9644\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 56s 531ms/step - loss: 0.0968 - acc: 0.9694 - val_loss: 0.1956 - val_acc: 0.9561\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 56s 526ms/step - loss: 0.0505 - acc: 0.9872 - val_loss: 0.2349 - val_acc: 0.9591\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 56s 531ms/step - loss: 0.0396 - acc: 0.9905 - val_loss: 0.2421 - val_acc: 0.9485\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 57s 539ms/step - loss: 0.0396 - acc: 0.9912 - val_loss: 0.2546 - val_acc: 0.9583\n",
            "Training and Prediction for the class:  refutation\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 58s 541ms/step - loss: 0.2683 - acc: 0.9551 - val_loss: 0.0708 - val_acc: 0.9894\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 58s 543ms/step - loss: 0.0892 - acc: 0.9733 - val_loss: 0.0638 - val_acc: 0.9894\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 58s 548ms/step - loss: 0.0473 - acc: 0.9852 - val_loss: 0.0736 - val_acc: 0.9894\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 59s 553ms/step - loss: 0.0448 - acc: 0.9840 - val_loss: 0.0723 - val_acc: 0.9894\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 59s 557ms/step - loss: 0.0449 - acc: 0.9896 - val_loss: 0.0776 - val_acc: 0.9894\n",
            "Training and Prediction for the class:  Favour\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 62s 568ms/step - loss: 1.0673 - acc: 0.6276 - val_loss: 0.6359 - val_acc: 0.6985\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 60s 570ms/step - loss: 0.4798 - acc: 0.7670 - val_loss: 0.6786 - val_acc: 0.6492\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 61s 574ms/step - loss: 0.3691 - acc: 0.8429 - val_loss: 0.7347 - val_acc: 0.6197\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 60s 570ms/step - loss: 0.2753 - acc: 0.8949 - val_loss: 0.7890 - val_acc: 0.5955\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 60s 570ms/step - loss: 0.2334 - acc: 0.9202 - val_loss: 0.8659 - val_acc: 0.5333\n",
            "Training and Prediction for the class:  oppose\n",
            "Epoch 1/5\n",
            "106/106 [==============================] - 63s 578ms/step - loss: 0.5737 - acc: 0.8870 - val_loss: 0.3180 - val_acc: 0.9083\n",
            "Epoch 2/5\n",
            "106/106 [==============================] - 60s 565ms/step - loss: 0.1920 - acc: 0.9368 - val_loss: 0.3310 - val_acc: 0.9023\n",
            "Epoch 3/5\n",
            "106/106 [==============================] - 60s 568ms/step - loss: 0.1274 - acc: 0.9550 - val_loss: 0.3750 - val_acc: 0.9023\n",
            "Epoch 4/5\n",
            "106/106 [==============================] - 60s 562ms/step - loss: 0.0956 - acc: 0.9649 - val_loss: 0.4155 - val_acc: 0.9053\n",
            "Epoch 5/5\n",
            "106/106 [==============================] - 61s 571ms/step - loss: 0.0770 - acc: 0.9747 - val_loss: 0.5152 - val_acc: 0.9053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3RemeYJj-AC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}